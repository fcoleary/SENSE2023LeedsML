{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lfj95/SENSE2023/blob/main/training_small.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evFEUBMkI2C_"
      },
      "source": [
        "# **Semantic segmentation of Aerial Imagery with U-Net-like architectures**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This notebook is a simple semantic segmentation program. The goal is to build and train a model which is able to automatically label each pixel in an aerial image with its semantic category. \n",
        "\n",
        "### The dataset we use consists of aerial imagery of Dubai obtained by MBRSC satellites and annotated with pixel-wise semantic segmentation in 6 classes. The total volume of the dataset is 1305 256*256 images grouped into 6 larger tiles. The classes are: Building, Land (unpaved area), Road, Vegetation, Water, Unlabeled."
      ],
      "metadata": {
        "id": "aau7A3P_zTeP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run on GPU\n",
        "\n",
        "For model runtime speed, check Collab is running on GPU. \n",
        "\n",
        "Click the “**Runtime**” dropdown menu. Select “**Change runtime type**”. Now select '**GPU**' in the “**Hardware accelerator**” dropdown menu. \n"
      ],
      "metadata": {
        "id": "_h92cmLhqytW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Python library\n",
        "```segmentation-models```: with Neural Networks for Image Segmentation based on Keras and TensorFlow."
      ],
      "metadata": {
        "id": "NoJxs142irKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U segmentation-models"
      ],
      "metadata": {
        "id": "8PwdU4LkpxPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Dataset from GitHub\n",
        "```git-clone``` - Clone a repository into a new directory"
      ],
      "metadata": {
        "id": "Cr5F0n0VizmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/lfj95/SENSE2023.git"
      ],
      "metadata": {
        "id": "-Xb1WtBWp7-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Packages"
      ],
      "metadata": {
        "id": "P9C4zTGSjKfZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NSUlcFM7L8l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "from keras import backend as K\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "from tensorflow.keras.metrics import MeanIoU\n",
        "from tensorflow.keras.utils import plot_model\n",
        "%env SM_FRAMEWORK=tf.keras\n",
        "import segmentation_models as sm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set the Current Working Directory"
      ],
      "metadata": {
        "id": "BHFCXaC3jspd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def makedir(new_dir):\n",
        "  if not os.path.exists(new_dir):\n",
        "    os.makedirs(new_dir)\n",
        "\n",
        "label_directory = os.getcwd() + \"/SENSE2023/split_data/labels.npy\"\n",
        "train_data_directory = os.getcwd() + \"/SENSE2023/split_data/train/images\"\n",
        "valid_data_directory = os.getcwd() + \"/SENSE2023/split_data/valid/images\"\n",
        "test_data_directory = os.getcwd() + \"/SENSE2023/split_data/test/images\"\n",
        "save_directory = os.getcwd() + \"/models\"\n",
        "makedir(save_directory)"
      ],
      "metadata": {
        "id": "92eqs4NmjvKh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM_djtz99ABc"
      },
      "source": [
        "### Read train/ validation/ test images from corresponding subdirectory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijixgRCV8MaZ"
      },
      "outputs": [],
      "source": [
        "def read_image(path):\n",
        "  image_dataset = []\n",
        "  images = os.listdir(path)  #List of all image names in this subdirectory\n",
        "  images = sorted(images)\n",
        "  for i, image_name in enumerate(images):  \n",
        "      if image_name.endswith(\".jpg\"):   #Only read jpg images...\n",
        "          image = cv2.imread(path+\"/\"+image_name, 1)  #Read each image as BGR\n",
        "          image_dataset.append(image)\n",
        "  return image_dataset\n",
        "\n",
        "X_train = np.array(read_image(train_data_directory))\n",
        "X_valid = np.array(read_image(valid_data_directory))\n",
        "X_test = np.array(read_image(test_data_directory))\n",
        "print(\"Train Images size:\", X_train.shape, '\\n', \"Validation Images size:\",X_valid.shape, '\\n', \"Test Images size:\",X_test.shape )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read array (integer values for each pixel) from .npy file,  to be used as labels. \n",
        "```\n",
        "tf.keras.utils.**to_categorical**(y, num_classes=None, dtype=\"float32\")\n",
        "```\n",
        "Converts a class vector (integers) to binary class matrix, e.g., for use with categorical_crossentropy."
      ],
      "metadata": {
        "id": "OPyg4mg60mt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dict_load=np.load(label_directory, allow_pickle=True)\n",
        "\n",
        "labels_train = dict_load.item()['labels_train']\n",
        "labels_valid = dict_load.item()['labels_valid']\n",
        "labels_test = dict_load.item()['labels_test']\n",
        "n_classes = len(np.unique(labels_train))\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "y_train = to_categorical(labels_train, num_classes=n_classes)\n",
        "y_valid = to_categorical(labels_valid, num_classes=n_classes)\n",
        "y_test = to_categorical(labels_test, num_classes=n_classes)\n"
      ],
      "metadata": {
        "id": "FYaMSEVG0pVR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the model: U-Net based model\n",
        "\n",
        "The model we defined consists of a contracting path and an expansive path. \n",
        "*   The contracting path follows the typical architecture of a convolutional network. It consists of the repeated application of two 3x3 convolutions, each followed by a rectified linear unit (ReLU) and a 2x2 max pooling operation with stride 2 for downsampling. \n",
        "*   At each downsampling step we double the number of feature channels. Every step in the expansive path consists of an upsampling of the feature map followed by a 2x2 convolution (“up-convolution”) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU. \n",
        "\n",
        "At the final layer, a 1x1 convolution is used to map each 64-component feature vector to the desired number of classes. \n",
        "\n",
        "Layers used: \n",
        "\n",
        "1.   [Conv2D](https://keras.io/api/layers/convolution_layers/convolution2d/) \n",
        "2.   [MaxPooling2D](https://keras.io/api/layers/pooling_layers/max_pooling2d/) \n",
        "3.  [UpSampling2D](https://keras.io/api/layers/reshaping_layers/up_sampling2d/) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nKPMpYYJFNiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  unet mini\n",
        "\n",
        "num_classes=6\n",
        "IMG_HEIGHT=256\n",
        "IMG_WIDTH=256\n",
        "IMG_CHANNELS=3\n",
        "\n",
        "inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
        "\n",
        "# contracting path\n",
        "c1 = Conv2D(16, 3, activation='relu', padding='same')(inputs)\n",
        "c1 = Conv2D(16, 3, activation='relu', padding='same')(c1)\n",
        "p1 = MaxPooling2D(pool_size=(2, 2))(c1)\n",
        "\n",
        "c2 = Conv2D(32, 3, activation='relu', padding='same')(p1)\n",
        "c2 = Conv2D(32, 3, activation='relu', padding='same')(c2)\n",
        "p2 = MaxPooling2D(pool_size=(2, 2))(c2)  \n",
        "  \n",
        "c3 = Conv2D(64, 3, activation='relu', padding='same')(p2)\n",
        "c3 = Conv2D(64, 3, activation='relu', padding='same')(c3)\n",
        "\n",
        "# expansive path\n",
        "c4 = UpSampling2D(size=(2, 2))(c3)\n",
        "c4 = concatenate([c4, c2])\n",
        "c4 = Conv2D(32, 3, activation='relu', padding='same')(c4)\n",
        "c4 = Conv2D(32, 3, activation='relu', padding='same')(c4)\n",
        "  \n",
        "c5 = UpSampling2D(size=(2, 2))(c4)\n",
        "c5 = concatenate([c5, c1])\n",
        "c5 = Conv2D(16, 3, activation='relu', padding='same')(c5)\n",
        "c5 = Conv2D(16, 3, activation='relu', padding='same')(c5)\n",
        "\n",
        "outputs = Conv2D(num_classes, 1, activation='softmax')(c5)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "# model.summary()\n"
      ],
      "metadata": {
        "id": "FU75hczsvTkg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model plotting utilities\n",
        "```\n",
        "tf.keras.utils.plot_model(model, to_file=\"model.png\", show_shapes=False, show_dtype=False, show_layer_names=True, rankdir=\"TB\", expand_nested=False, dpi=96, layer_range=None, show_layer_activations=False)\n",
        "```\n",
        "Converts a Keras model to dot format and save to a file."
      ],
      "metadata": {
        "id": "qGpZSxiXhEzc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vix-nM_nk4xF"
      },
      "outputs": [],
      "source": [
        "plot_model(model, to_file= save_directory + '/small_unet1.png', show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-avChET-F3K"
      },
      "source": [
        "### Configures the model for training.\n",
        "```\n",
        "compile(optimizer='rmsprop', loss=None, metrics=None, loss_weights=None, weighted_metrics=None, run_eagerly=None, steps_per_execution=None,jit_compile=None, **kwargs)\n",
        "```\n",
        "\n",
        "\n",
        "> *   optimizer:\tString (name of optimizer) or optimizer instance. See tf.keras.\n",
        "optimizers.\n",
        "*   loss: \tLoss function. Maybe be a string (name of loss function), or a tf.keras.losses.Loss instance. See tf.keras.losses.\n",
        "*   metrics: List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rjaeZCpD-ns9"
      },
      "outputs": [],
      "source": [
        "def jacard_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n",
        "metrics=['accuracy', jacard_coef]\n",
        "\n",
        "model.compile(optimizer='adam', loss=sm.losses.CategoricalCELoss(), metrics=metrics)\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model for a fixed number of epochs (iterations on a dataset).\n",
        "\n",
        "```\n",
        "model.fit(x=None, y=None, batch_size=None, epochs=1, verbose=\"auto\", callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False,)\n",
        "```\n",
        "\n",
        "\n",
        "> *    x: Input data. It could be either Numpy array(s) or TensorFlow tensor(s).\n",
        "*    y: Target data. Like the input data x, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x.\n",
        "*    batch_size: Integer or None. Number of samples per gradient update. If unspecified, batch_size will default to 32.\n",
        "*    epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided\n",
        "*    validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data.\n",
        "*    shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch').\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WsqMNhzoFlop"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p04oes0n-xML"
      },
      "outputs": [],
      "source": [
        "history1 = model.fit(X_train, y_train,\n",
        "                    batch_size = 16, \n",
        "                    verbose=1, \n",
        "                    epochs=60, \n",
        "                    validation_data=(X_valid, y_valid), \n",
        "                    shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmIAcJlMOftT"
      },
      "outputs": [],
      "source": [
        "model.save(save_directory + '/models/small_unet1.hdf5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEWKGcxj_YoK"
      },
      "source": [
        "### Plot the training and validation accuracy and loss at each epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLeHDuWx_WXn"
      },
      "outputs": [],
      "source": [
        "history = history1\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'y', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "acc = history.history['jacard_coef']\n",
        "val_acc = history.history['val_jacard_coef']\n",
        "\n",
        "plt.plot(epochs, acc, 'y', label='Training IoU')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation IoU')\n",
        "plt.title('Training and validation IoU')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('IoU')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg_IQmIfCxe_"
      },
      "source": [
        "### Visualize predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61-QaRRoCt7n"
      },
      "outputs": [],
      "source": [
        "test_img_number = random.randint(0, len(X_test))\n",
        "test_img = X_test[test_img_number]\n",
        "ground_truth = np.argmax(y_test, axis = 3)[test_img_number]\n",
        "#test_img_norm=test_img[:,:,0][:,:,None]\n",
        "test_img_input=np.expand_dims(test_img, 0)\n",
        "prediction = (model.predict(test_img_input))\n",
        "predicted_img=np.argmax(prediction, axis=3)[0,:,:]\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.subplot(231)\n",
        "plt.title('Testing Image')\n",
        "plt.imshow(test_img)\n",
        "plt.subplot(232)\n",
        "plt.title('Testing Label')\n",
        "plt.imshow(ground_truth, vmin=0, vmax=5)\n",
        "plt.subplot(233)\n",
        "plt.title('Prediction on test image')\n",
        "plt.imshow(predicted_img, vmin=0, vmax=5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercises 1. Finetune the segmentation model \n",
        "\n",
        "Now try adjusting the model by implementing different methods of reducing overfitting and increasing the model's ability to generalise.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gvHiAZdmVTnt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Retrain the model by using or adding a different loss function and see what happens to the performance of the segmentation model. The segmentation models losses can be combined together by '+' and scaled by integer or float factor set class weights for dice_loss.\n",
        "E.g., add in a **CategoricalFocalLoss**/**dice_loss** in the Configure the model for training section.\n",
        "\n",
        " >Hint: Further details for loss options can be found in [segmentation losses](https://segmentation-models.readthedocs.io/en/latest/api.html?highlight=loss#losses)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ri3N2us-qThR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.   In the previous model, the contracting path follows the formula: \n",
        "\n",
        "  `conv_layer1 -> conv_layer2 -> max_pooling` \n",
        "  \n",
        "  In the expansive path, the image is going to be upsized to its original size. The formula follows:\n",
        "\n",
        "  `UpSampling2D -> concatenate -> conv_layer1 -> conv_layer2` \n",
        "\n",
        "  We built 2 contracting blocks and 2 expanding blocks. Could you try lessening the block or repeating the block more times? Does this make the segmentation results worse or better?\n",
        "  Beside, you can also explore removing one of the two convolutions in every contracting/expansive block.\n",
        "\n",
        "  > Hint: Make sure the start and end image dimensions match, check the UNET structure in you’ve created small_unet1.png\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Xo98nXxzryrm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.   Change the CONV layers with Batch Normalization & Dropout. \n",
        "\n",
        "\n",
        "*   One common issue in training a Segmentation model based on Deep Convolutional Neural Network for is **the internal covariate shift**, where the training of convolutional kernels is encumbered by the distribution change of input features, hence both the training speed and performance are decreased. Batch Normalization is the first proposed method for addressing internal\n",
        "covariate shift and is widely used. \n",
        "*   Since **parameter size** in some Conv layers are quite large, and the training data is small. It is reasonable to add dropout between larg convolutional connections. \n",
        "\n",
        "  > Hint: \n",
        "  *   Keras Implementation of [Batch normalisation](https://keras.io/api/layers/normalization_layers/batch_normalization/)\n",
        "  *   Keras Implementation of [Dropout](https://keras.io/api/layers/regularization_layers/dropout/)\n",
        "\n"
      ],
      "metadata": {
        "id": "ic72xG8wsYTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.   Data augmentation of your choice. \n",
        "\n",
        "  > Hint: **scipy.ndimage** module provides a powerful set of general, n-dimensional image processing operations. [Interpolation in scipy.ndimage](https://docs.scipy.org/doc/scipy-0.14.0/reference/ndimage.html#module-scipy.ndimage.interpolation) include basic operations like shifts, rotations and resizing as well as more general coordinate transformations like affine transforms or warping. See the following rotate example: \n",
        "\n",
        "  ```\n",
        "scipy.ndimage.interpolation.rotate(input, angle, axes=(1, 0), reshape=True, output=None, order=3, mode='constant', cval=0.0, prefilter=True)\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KFmn416SsYKK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43Ze2TYw_J-v"
      },
      "source": [
        "### Exercises 2.Try another model\n",
        "Change the network architecture to U-net with Resnet backbone. Use pretrainded weights to initialize it.\n",
        "\n",
        "> Hint: To do this, follow the instructions in [Segmentation Models Python API](https://segmentation-models.readthedocs.io/en/latest/tutorial.html?highlight=loss#simple-training-pipeline), and you can implete U-net with pretrained weights in simple two lines. It provides 4 models architectures for binary and multi class segmentation (including legendary Unet) and 25 available backbones for each architecture.  All backbones have pre-trained weights for faster and better convergence.\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "training_small.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}